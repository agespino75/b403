{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "465c003a-29cf-4cfe-a0c6-d36c04ae2b37",
   "metadata": {},
   "source": [
    "# Main built-in LCEL Runnables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d46ff75-72ab-4391-904f-459bae77fc7a",
   "metadata": {},
   "source": [
    "## Contents\n",
    "* RunnablePassthrough.\n",
    "* RunnableLambda.\n",
    "* RunnableParallel.\n",
    "    * itemgetter.\n",
    "* RunnableBranch. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be46161e-45e9-46d7-8214-bcbea10aff2e",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871e0018-cba4-4959-881a-0a65093d202d",
   "metadata": {},
   "source": [
    "#### After you download the code from the github repository in your computer\n",
    "In terminal:\n",
    "* cd project_name\n",
    "* pyenv local 3.11.4\n",
    "* poetry install\n",
    "* poetry shell\n",
    "\n",
    "#### To open the notebook with Jupyter Notebooks\n",
    "In terminal:\n",
    "* jupyter lab\n",
    "\n",
    "Go to the folder of notebooks and open the right notebook.\n",
    "\n",
    "#### To see the code in Virtual Studio Code or your editor of choice.\n",
    "* open Virtual Studio Code or your editor of choice.\n",
    "* open the project-folder\n",
    "* open the 005-builtin-runnables.py file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5fbb27-418c-4938-a82f-a40a01cba0ee",
   "metadata": {},
   "source": [
    "## Create your .env file\n",
    "* In the github repo we have included a file named .env.example\n",
    "* Rename that file to .env file and here is where you will add your confidential api keys. Remember to include:\n",
    "* OPENAI_API_KEY=your_openai_api_key\n",
    "* LANGCHAIN_TRACING_V2=true\n",
    "* LANGCHAIN_ENDPOINT=https://api.smith.langchain.com\n",
    "* LANGCHAIN_API_KEY=your_langchain_api_key\n",
    "* LANGCHAIN_PROJECT=your_project_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863dd299-0780-49ad-a1b7-b76e249350da",
   "metadata": {},
   "source": [
    "We will call our LangSmith project **005-builtin-runnables**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b97622d-b4bb-4ea2-9bc3-9b72beaae588",
   "metadata": {},
   "source": [
    "## Connect with the .env file located in the same directory of this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5008faa2-7f0f-45a5-8cd7-8e73114fdc35",
   "metadata": {},
   "source": [
    "If you are using the pre-loaded poetry shell, you do not need to install the following package because it is already pre-loaded for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e3f063-722c-4716-b794-4600a066e767",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "id": "fecd39d0-e72e-4bc2-8a68-2fa4008ea365",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T04:50:40.152182Z",
     "start_time": "2025-06-15T04:50:40.131773Z"
    }
   },
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "openai_api_key = os.environ[\"OPENAI_API_KEY\"]"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "id": "03f4a923-b19e-498e-9be5-e47ec4a77d80",
   "metadata": {},
   "source": [
    "#### Install LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe0e9eb-1c55-4cee-ba20-be37a5a80c01",
   "metadata": {},
   "source": [
    "If you are using the pre-loaded poetry shell, you do not need to install the following package because it is already pre-loaded for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1cf94ae-6c39-4475-9c5b-4b74d8d78753",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189e9e17-dfb0-4fd3-85b9-1fba83771941",
   "metadata": {},
   "source": [
    "## Connect with an LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffd6f1e-3a7f-4695-afd5-e3dd57f13f86",
   "metadata": {},
   "source": [
    "If you are using the pre-loaded poetry shell, you do not need to install the following package because it is already pre-loaded for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "148df8e0-361d-4ddd-8709-af48fa1648d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain-openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1998155-91de-4cbc-bc88-8d77beefb51b",
   "metadata": {},
   "source": [
    "* NOTE: Since right now is the best LLM in the market, we will use OpenAI by default. You will see how to connect with other Open Source LLMs like Llama3 or Mistral in a next lesson."
   ]
  },
  {
   "cell_type": "code",
   "id": "6ae8595e-5c07-4b02-8a79-db55fd357527",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T04:50:45.780984Z",
     "start_time": "2025-06-15T04:50:45.774059Z"
    }
   },
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "id": "fffe937d-dce4-43e7-a647-7a86dbd37923",
   "metadata": {},
   "source": [
    "## RunnablePassthrough\n",
    "* It does not do anything to the input data.\n",
    "* Let's see it in a very simple example: a chain with just RunnablePassthrough() will output the original input without any modification."
   ]
  },
  {
   "cell_type": "code",
   "id": "fb318d05-70ee-4f57-81a3-9a512f10bc33",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T03:18:49.542357Z",
     "start_time": "2025-06-15T03:18:49.525460Z"
    }
   },
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "chain = RunnablePassthrough()"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "d0a4b9de-a4b3-4687-82b7-dcfe910f23de",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T03:18:51.846967Z",
     "start_time": "2025-06-15T03:18:51.836542Z"
    }
   },
   "source": [
    "chain.invoke(\"Abram\")"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Abram'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "id": "f94758c3-95fc-465b-aec1-8048ab161d45",
   "metadata": {},
   "source": [
    "## RunnableLambda\n",
    "* To use a custom function inside a LCEL chain we need to wrap it up with RunnableLambda.\n",
    "* Let's define a very simple function to create Russian lastnames:"
   ]
  },
  {
   "cell_type": "code",
   "id": "4f3438c0-05c3-48ab-a1eb-60209717cfe6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T03:18:03.673161Z",
     "start_time": "2025-06-15T03:18:03.670001Z"
    }
   },
   "source": [
    "def russian_lastname(name: str) -> str:\n",
    "    return f\"{name}ovich\""
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "681c8c84-94b5-4fcc-910b-915db65bc9a7",
   "metadata": {},
   "source": [
    "* In order to use this function inside of a LCEL chain, we will use RunnableLambda():"
   ]
  },
  {
   "cell_type": "code",
   "id": "26bbdd15-32b8-4594-9395-a33c74e95fe0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T03:19:02.763479Z",
     "start_time": "2025-06-15T03:19:02.758863Z"
    }
   },
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "#chain = RunnablePassthrough() | RunnableLambda(russian_lastname)\n",
    "chain = RunnableLambda(russian_lastname)"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "3e9bdaa7-0cc8-49f8-9aeb-b0ddece0d55e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T03:19:06.086171Z",
     "start_time": "2025-06-15T03:19:06.080894Z"
    }
   },
   "source": [
    "chain.invoke(\"Abram\")"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Abramovich'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "id": "db9b8f10-0138-432f-b694-8a5360422221",
   "metadata": {},
   "source": [
    "* As you see, our chain is now applying the russian_lastname function to our input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95190509-301b-4be2-9b8b-7dc17193bfee",
   "metadata": {},
   "source": [
    "## RunnableParallel\n",
    "* We will use RunnableParallel() for running tasks in parallel.\n",
    "* This is probably the most important and most useful Runnable from LangChain.\n",
    "* In the following chain, RunnableParallel is going to run these two tasks in parallel:\n",
    "    * operation_a will use RunnablePassthrough.\n",
    "    * operation_b will use RunnableLambda with the russian_lastname function."
   ]
  },
  {
   "cell_type": "code",
   "id": "9c6bef0b-399d-43b8-9455-b7f2a9050172",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T03:22:05.807406Z",
     "start_time": "2025-06-15T03:22:05.797087Z"
    }
   },
   "source": [
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "chain = RunnableParallel(\n",
    "    {\n",
    "        \"operation_a\": RunnablePassthrough(),\n",
    "        \"operation_b\": RunnableLambda(russian_lastname)\n",
    "    }\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "id": "38ac0a07-6bec-4cc3-9049-b1ea49d560b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T03:22:23.856987Z",
     "start_time": "2025-06-15T03:22:23.833051Z"
    }
   },
   "source": [
    "chain.invoke(\"Abram\")"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'operation_a': 'Abram', 'operation_b': 'Abramovich'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "id": "9b5372a0-a33e-4a02-9f84-43292bcde994",
   "metadata": {},
   "source": [
    "* Instead of using RunnableLambda, now we are going to use a lambda function and we will invoke the chain with two inputs:"
   ]
  },
  {
   "cell_type": "code",
   "id": "971fa8a7-5866-4716-9c4f-67260283c8ff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T03:27:23.344547Z",
     "start_time": "2025-06-15T03:27:23.327846Z"
    }
   },
   "source": [
    "chain = RunnableParallel(\n",
    "    {\n",
    "        \"operation_a\": RunnablePassthrough(),\n",
    "        \"soccer_player\": lambda x: x[\"name\"]+\"ovich\"\n",
    "    }\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "id": "95b45b1a-6352-4618-a5e3-6cbda7236404",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T03:28:21.813541Z",
     "start_time": "2025-06-15T03:28:21.788190Z"
    }
   },
   "source": [
    "chain.invoke({\n",
    "    \"name1\": \"Jordan\",\n",
    "    \"name\": \"Abram\"\n",
    "})"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'operation_a': {'name1': 'Jordan', 'name': 'Abram'},\n",
       " 'soccer_player': 'Abramovich'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "id": "90e3cd68-c918-4b4d-9769-bbaee8adb208",
   "metadata": {},
   "source": [
    "* See how the lambda function is taking the \"name\" input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7891ee-c48c-4a26-8b24-1465a43f774e",
   "metadata": {},
   "source": [
    "#### We can add more Runnables to the chain\n",
    "* In the following example, the prompt Runnable will take the output of the RunnableParallel:"
   ]
  },
  {
   "cell_type": "code",
   "id": "f786101f-a555-445f-8c26-29d9b65a37e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T03:30:12.067800Z",
     "start_time": "2025-06-15T03:30:12.002028Z"
    }
   },
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "id": "dece6a57-ef92-4ca8-af7c-cfbd0b9a03f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T03:30:17.244289Z",
     "start_time": "2025-06-15T03:30:17.239787Z"
    }
   },
   "source": [
    "prompt = ChatPromptTemplate.from_template(\"tell me a curious fact about {soccer_player}\")\n",
    "\n",
    "output_parser = StrOutputParser()"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "id": "dea6be27-c67f-4bf0-a3db-99d716778486",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T03:30:31.184840Z",
     "start_time": "2025-06-15T03:30:31.181454Z"
    }
   },
   "source": [
    "def russian_lastname_from_dictionary(person):\n",
    "    return person[\"name\"] + \"ovich\""
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "id": "2a234758-44f5-4bdc-a1ea-ff972e23ff5d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T03:38:50.216256Z",
     "start_time": "2025-06-15T03:38:50.207714Z"
    }
   },
   "source": [
    "chain = RunnableParallel(\n",
    "    {\n",
    "        \"operation_a\": RunnablePassthrough(),\n",
    "        \"soccer_player\": RunnableLambda(russian_lastname_from_dictionary),\n",
    "        \"operation_c\": RunnablePassthrough(),\n",
    "    }\n",
    ") | prompt | model | output_parser"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "id": "83889244-92be-4d35-a439-82c0949ccdf2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T03:38:57.295312Z",
     "start_time": "2025-06-15T03:38:54.852607Z"
    }
   },
   "source": [
    "chain.invoke({\n",
    "    \"name1\": \"Jordan\",\n",
    "    \"name\": \"Abram\"\n",
    "})"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A curious fact about Roman Abramovich is that he is not only known for his ownership of Chelsea Football Club but also for his diverse interests outside of football. In addition to being a prominent businessman and politician in Russia, Abramovich has an impressive collection of art, including works by renowned artists such as Claude Monet and Alberto Giacometti. His passion for art led him to establish the Garage Museum of Contemporary Art in Moscow, further showcasing his commitment to supporting the art community. This aspect of his life often goes unnoticed amid his high-profile football dealings.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "id": "14c1f143-f7eb-4f86-9f49-e4dc8c249451",
   "metadata": {},
   "source": [
    "* As you saw, the prompt Runnable took \"Abramovich\", the output of the RunnableParallel, as the value for the \"soccer_player\" variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78f9c36-ff04-4ac2-9794-503b021018dd",
   "metadata": {},
   "source": [
    "## Let's see a more advanced use of RunnableParallel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf504a95-c3d3-4fd5-a0b1-58d4618cb8fc",
   "metadata": {},
   "source": [
    "If you are using the pre-loaded poetry shell, you do not need to install the following packages because they are already pre-loaded for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89820004-06e2-47f9-a53a-52d9753286ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9048e7b-1e71-4cbc-bb7a-2c66543d6889",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "id": "999c0201-35df-4ebf-b46e-be0ef40f672c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T03:42:25.827484Z",
     "start_time": "2025-06-15T03:42:23.131578Z"
    }
   },
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "vectorstore = FAISS.from_texts(\n",
    "    [\"AI Accelera has trained more than 10,000 Alumni from all continents and top companies\"], embedding=OpenAIEmbeddings()\n",
    ")\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "retrieval_chain = (\n",
    "    RunnableParallel({\"context\": retriever, \"question\": RunnablePassthrough()})\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "retrieval_chain.invoke(\"who are the Alumni of AI Accelera?\")"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Alumni of AI Accelera are individuals from all continents and top companies who have been trained by the organization.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "id": "6bb9fdb9-f85f-4b14-9cd4-7cf3f841295b",
   "metadata": {},
   "source": [
    "#### Important: the syntax of RunnableParallel can have several variations.\n",
    "* When composing a RunnableParallel with another Runnable you do not need to wrap it up in the RunnableParallel class. Inside a chain, the next three syntaxs are equivalent:\n",
    "    * `RunnableParallel({\"context\": retriever, \"question\": RunnablePassthrough()})`\n",
    "    * `RunnableParallel(context=retriever, question=RunnablePassthrough())`\n",
    "    * `{\"context\": retriever, \"question\": RunnablePassthrough()}`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010288fe-3891-4d9d-952b-c9c31c27e95c",
   "metadata": {},
   "source": [
    "## Using itemgetter with RunnableParallel\n",
    "* When you are calling the LLM with several different input variables."
   ]
  },
  {
   "cell_type": "code",
   "id": "e1d721b7-6208-47c4-8a51-0104329a849d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T04:42:24.898006Z",
     "start_time": "2025-06-15T04:42:22.508996Z"
    }
   },
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "vectorstore = FAISS.from_texts(\n",
    "    [\"AI Accelera has trained more than 5,000 Enterprise Alumni.\"], embedding=OpenAIEmbeddings()\n",
    ")\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer in the following language: {language}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        \"context\": itemgetter(\"question\") | retriever,\n",
    "        \"question\": itemgetter(\"question\"),\n",
    "        \"language\": itemgetter(\"language\"),\n",
    "    }\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "chain.invoke({\"question\": \"How many Enterprise Alumni has trained AI Accelera?\", \"language\": \"Pirate English\"})"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Arrr, matey! AI Accelera has trained over 5,000 Enterprise Alumni, savvy?'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "id": "5a50a4a2-8a4a-4f6a-ab4b-ac45d3cdb228",
   "metadata": {},
   "source": [
    "## RunnableBranch: Router Chain\n",
    "* A RunnableBranch is a special type of runnable that allows you to define a set of conditions and runnables to execute based on the input.\n",
    "* **A RunnableBranch is initialized with a list of (condition, runnable) pairs and a default runnable**. It selects which branch by passing each condition the input it's invoked with. It selects the first condition to evaluate to True, and runs the corresponding runnable to that condition with the input.\n",
    "* For advanced uses, a [custom function](https://python.langchain.com/v0.1/docs/expression_language/how_to/routing/) may be a better alternative than RunnableBranch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0108154-e304-4aa2-aa97-f64529a81ade",
   "metadata": {},
   "source": [
    "The following advanced example can classify and respond to user questions based on specific topics like rock, politics, history, sports, or general inquiries. **It uses some new topics that we will explain in the following lesson**. Here’s a simplified explanation of each part:\n",
    "\n",
    "1. **Prompt Templates**: Each template is tailored for a specific topic:\n",
    "   - **rock_template**: Configured for rock and roll related questions.\n",
    "   - **politics_template**: Tailored to answer questions about politics.\n",
    "   - **history_template**: Designed for queries related to history.\n",
    "   - **sports_template**: Set up to address sports-related questions.\n",
    "   - **general_prompt**: A general template for queries that don't fit the specific categories.\n",
    "\n",
    "   Each template includes a placeholder `{input}` where the actual user question will be inserted.\n",
    "\n",
    "2. **RunnableBranch**: This is a branching mechanism that selects which template to use based on the topic of the question. It evaluates conditions (like `x[\"topic\"] == \"rock\"`) to determine the topic and uses the appropriate prompt template.\n",
    "\n",
    "3. **Topic Classifier**: A Pydantic class that classifies the topic of a user's question into one of the predefined categories (rock, politics, history, sports, or general).\n",
    "\n",
    "4. **Classifier Chain**:\n",
    "   - **Chain**: Processes the user's input to predict the topic.\n",
    "   - **Parser**: Extracts the predicted topic from the classifier's output.\n",
    "\n",
    "5. **RunnablePassthrough**: This component feeds the user's input and the classified topic into the RunnableBranch.\n",
    "\n",
    "6. **Final Chain**:\n",
    "   - The user's input is first processed to classify its topic.\n",
    "   - The appropriate prompt is then selected based on the classified topic.\n",
    "   - The selected prompt is used to formulate a question which is then sent to a model (like ChatOpenAI).\n",
    "   - The model’s response is parsed as a string and returned.\n",
    "\n",
    "7. **Execution**:\n",
    "   - The chain is invoked with a sample question, \"Who was Napoleon Bonaparte?\" \n",
    "   - Based on the classification, it selects the appropriate template, generates a query to the chat model, and processes the response.\n",
    "\n",
    "The system effectively creates a dynamic response generator that adjusts the way it answers based on the topic of the inquiry, making use of specialized knowledge for different subjects."
   ]
  },
  {
   "cell_type": "code",
   "id": "3d269f55-2a46-4855-b5bc-095027386c01",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T04:11:21.410949Z",
     "start_time": "2025-06-15T04:11:21.333286Z"
    }
   },
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "rock_template = \"\"\"You are a very smart rock and roll professor. \\\n",
    "You are great at answering questions about rock and roll in a concise\\\n",
    "and easy to understand manner.\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "rock_prompt = PromptTemplate.from_template(rock_template)\n",
    "\n",
    "politics_template = \"\"\"You are a very good politics professor. \\\n",
    "You are great at answering politics questions..\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "politics_prompt = PromptTemplate.from_template(politics_template)\n",
    "\n",
    "history_template = \"\"\"You are a very good history teacher. \\\n",
    "You have an excellent knowledge of and understanding of people,\\\n",
    "events and contexts from a range of historical periods.\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "history_prompt = PromptTemplate.from_template(history_template)\n",
    "\n",
    "sports_template = \"\"\" You are a sports teacher.\\\n",
    "You are great at answering sports questions.\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "sports_prompt = PromptTemplate.from_template(sports_template)"
   ],
   "outputs": [],
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "id": "6083a779-e9fe-4dbf-9e0b-ead57ddfa3d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T04:12:20.993851Z",
     "start_time": "2025-06-15T04:12:20.951554Z"
    }
   },
   "source": [
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import RunnableBranch"
   ],
   "outputs": [],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "id": "8fa09888-ee51-4f43-91c5-0f3136ce385d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T04:13:07.394631Z",
     "start_time": "2025-06-15T04:13:07.390114Z"
    }
   },
   "source": [
    "general_prompt = PromptTemplate.from_template(\n",
    "    \"You are a helpful assistant. Answer the question as accurately as you can.\\n\\n{input}\"\n",
    ")\n",
    "prompt_branch = RunnableBranch(\n",
    "  (lambda x: x[\"topic\"] == \"rock\", rock_prompt),\n",
    "  (lambda x: x[\"topic\"] == \"politics\", politics_prompt),\n",
    "  (lambda x: x[\"topic\"] == \"history\", history_prompt),\n",
    "  (lambda x: x[\"topic\"] == \"sports\", sports_prompt),\n",
    "  general_prompt\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "id": "126b47e6-e3f4-4374-8f20-c0a884377d73",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T04:58:10.207017Z",
     "start_time": "2025-06-15T04:58:10.140350Z"
    }
   },
   "source": [
    "from typing import Literal\n",
    "\n",
    "from langchain.pydantic_v1 import BaseModel\n",
    "from langchain.output_parsers.openai_functions import PydanticAttrOutputFunctionsParser\n",
    "from langchain_core.utils.function_calling import convert_to_openai_function\n",
    "\n",
    "class TopicClassifier(BaseModel):\n",
    "    \"Classify the topic of the user question\"\n",
    "    \n",
    "    topic: Literal[\"rock\", \"politics\", \"history\", \"sports\"]\n",
    "    \"The topic of the user question. One of 'rock', 'politics', 'history', 'sports' or 'general'.\"\n",
    "\n",
    "classifier_function = convert_to_openai_function(TopicClassifier)\n",
    "\n",
    "llm = ChatOpenAI().bind(functions=[classifier_function], function_call={\"name\": \"TopicClassifier\"}) \n",
    "\n",
    "#parser = PydanticAttrOutputFunctionsParser(pydantic_schema=TopicClassifier, attr_name=\"topic\")\n",
    "from langchain.pydantic_v1 import BaseModel\n",
    "from __main__ import TopicClassifier\n",
    "\n",
    "print(issubclass(TopicClassifier, BaseModel))  # ✅ debe decir True\n",
    "print(type(TopicClassifier))  # ✅ debe mostrar: <class 'langchain.pydantic_v1.main.ModelMetaclass'>\n",
    "\n",
    "parser = PydanticAttrOutputFunctionsParser(pydantic_schema=TopicClassifier, attr_name=\"topic\")\n",
    "\n",
    "classifier_chain = llm | parser"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "<class 'pydantic.v1.main.ModelMetaclass'>\n"
     ]
    },
    {
     "ename": "ValidationError",
     "evalue": "2 validation errors for PydanticAttrOutputFunctionsParser\npydantic_schema.is-subclass[BaseModel]\n  Input should be a subclass of BaseModel [type=is_subclass_of, input_value=<class '__main__.TopicClassifier'>, input_type=ModelMetaclass]\n    For further information visit https://errors.pydantic.dev/2.11/v/is_subclass_of\npydantic_schema.dict[str,is-subclass[BaseModel]]\n  Input should be a valid dictionary [type=dict_type, input_value=<class '__main__.TopicClassifier'>, input_type=ModelMetaclass]\n    For further information visit https://errors.pydantic.dev/2.11/v/dict_type",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mValidationError\u001B[39m                           Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[18]\u001B[39m\u001B[32m, line 24\u001B[39m\n\u001B[32m     21\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;28missubclass\u001B[39m(TopicClassifier, BaseModel))  \u001B[38;5;66;03m# ✅ debe decir True\u001B[39;00m\n\u001B[32m     22\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;28mtype\u001B[39m(TopicClassifier))  \u001B[38;5;66;03m# ✅ debe mostrar: <class 'langchain.pydantic_v1.main.ModelMetaclass'>\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m24\u001B[39m parser = \u001B[43mPydanticAttrOutputFunctionsParser\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpydantic_schema\u001B[49m\u001B[43m=\u001B[49m\u001B[43mTopicClassifier\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattr_name\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mtopic\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m     26\u001B[39m classifier_chain = llm | parser\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/langchain_core/load/serializable.py:130\u001B[39m, in \u001B[36mSerializable.__init__\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m    128\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, *args: Any, **kwargs: Any) -> \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    129\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"\"\"\"\u001B[39;00m  \u001B[38;5;66;03m# noqa: D419\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m130\u001B[39m     \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[34;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/pydantic/main.py:253\u001B[39m, in \u001B[36mBaseModel.__init__\u001B[39m\u001B[34m(self, **data)\u001B[39m\n\u001B[32m    251\u001B[39m \u001B[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001B[39;00m\n\u001B[32m    252\u001B[39m __tracebackhide__ = \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m253\u001B[39m validated_self = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m__pydantic_validator__\u001B[49m\u001B[43m.\u001B[49m\u001B[43mvalidate_python\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mself_instance\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m    254\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m validated_self:\n\u001B[32m    255\u001B[39m     warnings.warn(\n\u001B[32m    256\u001B[39m         \u001B[33m'\u001B[39m\u001B[33mA custom validator is returning a value other than `self`.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m'\u001B[39m\n\u001B[32m    257\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mReturning anything other than `self` from a top level model validator isn\u001B[39m\u001B[33m'\u001B[39m\u001B[33mt supported when validating via `__init__`.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m\n\u001B[32m    258\u001B[39m         \u001B[33m'\u001B[39m\u001B[33mSee the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.\u001B[39m\u001B[33m'\u001B[39m,\n\u001B[32m    259\u001B[39m         stacklevel=\u001B[32m2\u001B[39m,\n\u001B[32m    260\u001B[39m     )\n",
      "\u001B[31mValidationError\u001B[39m: 2 validation errors for PydanticAttrOutputFunctionsParser\npydantic_schema.is-subclass[BaseModel]\n  Input should be a subclass of BaseModel [type=is_subclass_of, input_value=<class '__main__.TopicClassifier'>, input_type=ModelMetaclass]\n    For further information visit https://errors.pydantic.dev/2.11/v/is_subclass_of\npydantic_schema.dict[str,is-subclass[BaseModel]]\n  Input should be a valid dictionary [type=dict_type, input_value=<class '__main__.TopicClassifier'>, input_type=ModelMetaclass]\n    For further information visit https://errors.pydantic.dev/2.11/v/dict_type"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T04:55:38.459799Z",
     "start_time": "2025-06-15T04:55:37.762466Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from typing import Literal\n",
    "from pydantic import BaseModel\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.utils.function_calling import convert_to_openai_function\n",
    "\n",
    "# ✅ Modelo Pydantic v2\n",
    "class TopicClassifier(BaseModel):\n",
    "    \"\"\"Classify the topic of the user question\"\"\"\n",
    "    topic: Literal[\"rock\", \"politics\", \"history\", \"sports\"]\n",
    "\n",
    "# ✅ Conversión a función de OpenAI\n",
    "classifier_function = convert_to_openai_function(TopicClassifier)\n",
    "\n",
    "# ✅ Modelo LLM configurado para usar la función\n",
    "llm = ChatOpenAI().bind(\n",
    "    functions=[classifier_function],\n",
    "    function_call={\"name\": \"TopicClassifier\"}\n",
    ")\n",
    "\n",
    "# ✅ Ejecución\n",
    "response = llm.invoke(\"Tell me about the Cold War.\")\n",
    "\n",
    "# ✅ Extracción manual del resultado\n",
    "args = response.additional_kwargs.get(\"function_call\", {}).get(\"arguments\")\n",
    "import json\n",
    "parsed_args = json.loads(args)\n",
    "result = TopicClassifier.model_validate(parsed_args)\n",
    "\n",
    "print(result.topic)  # Output esperado: \"history\"\n"
   ],
   "id": "7c40cdc260386768",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "history\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "id": "6f825ac1-df8b-4a6d-a24d-036b4c758e7b",
   "metadata": {},
   "source": [
    "The `classifier_function` classifies or categorizes the topic of a user's question into specific categories such as \"rock,\" \"politics,\" \"history,\" or \"sports.\" Here’s how it works in simple terms:\n",
    "\n",
    "1. **Conversion to Function**: It converts the `TopicClassifier` Pydantic class, which is a predefined classification system, into a function that can be easily used with LangChain. This conversion process involves wrapping the class so that it can be integrated and executed within an OpenAI model.\n",
    "\n",
    "2. **Topic Detection**: When you input a question, this function analyzes the content of the question to determine which category or topic it belongs to. It looks for keywords or patterns that match specific topics. For example, if the question is about a rock band, the classifier would identify the topic as \"rock.\"\n",
    "\n",
    "3. **Output**: The function outputs the identified topic as a simple label, like \"rock\" or \"history.\" This label is then used by other parts of the LangChain to decide how to handle the question, such as choosing the right template for formulating a response.\n",
    "\n",
    "In essence, the `classifier_function` acts as a smart filter that helps the system understand what kind of question is being asked so that it can respond more accurately and relevantly."
   ]
  },
  {
   "cell_type": "code",
   "id": "57438410-02c8-4808-abeb-1f941a7fafe5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T04:18:42.233118Z",
     "start_time": "2025-06-15T04:18:42.160883Z"
    }
   },
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "\n",
    "final_chain = (\n",
    "    RunnablePassthrough.assign(topic=itemgetter(\"input\") | classifier_chain) \n",
    "    | prompt_branch \n",
    "    | ChatOpenAI()\n",
    "    | StrOutputParser()\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "id": "beaf2559-318a-4d84-ae14-6582232b19e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T04:25:10.412207Z",
     "start_time": "2025-06-15T04:25:09.060950Z"
    }
   },
   "source": [
    "final_chain.invoke(\n",
    "    {\"input\":\"Who is Matei Zaharia \" }\n",
    ")"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Matei Zaharia is a Romanian-Canadian computer scientist and co-founder of Databricks, a company that provides a unified analytics platform. He is also the creator of Apache Spark, an open-source cluster computing framework. Zaharia is known for his work in big data processing and machine learning, and has made significant contributions to the field of distributed computing.'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 38
  },
  {
   "cell_type": "markdown",
   "id": "9521a3fe-c9a0-4e29-aad4-027fb6659d98",
   "metadata": {},
   "source": [
    "## How to execute the code from Visual Studio Code\n",
    "* In Visual Studio Code, see the file 005-builtin-runnables.py\n",
    "* In terminal, make sure you are in the directory of the file and run:\n",
    "    * python 005-builtin-runnables.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aab9741-4f52-4220-ac4d-ff37035cddbe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

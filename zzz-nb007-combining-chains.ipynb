{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d733e1a-fff5-4739-b702-cc4af0a69d41",
   "metadata": {},
   "source": [
    "# Combining LCEL Chains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48386f20-c929-48a9-8720-0953fcd67dd0",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65ee060-21f2-4e01-b283-1fd656dac1e9",
   "metadata": {},
   "source": [
    "#### After you download the code from the github repository in your computer\n",
    "In terminal:\n",
    "* cd project_name\n",
    "* pyenv local 3.11.4\n",
    "* poetry install\n",
    "* poetry shell\n",
    "\n",
    "#### To open the notebook with Jupyter Notebooks\n",
    "In terminal:\n",
    "* jupyter lab\n",
    "\n",
    "Go to the folder of notebooks and open the right notebook.\n",
    "\n",
    "#### To see the code in Virtual Studio Code or your editor of choice.\n",
    "* open Virtual Studio Code or your editor of choice.\n",
    "* open the project-folder\n",
    "* open the 007-main-ops-lcel-chain.py file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18edd454-6d12-445d-b4f9-b228097a1724",
   "metadata": {},
   "source": [
    "## Create your .env file\n",
    "* In the github repo we have included a file named .env.example\n",
    "* Rename that file to .env file and here is where you will add your confidential api keys. Remember to include:\n",
    "* OPENAI_API_KEY=your_openai_api_key\n",
    "* LANGCHAIN_TRACING_V2=true\n",
    "* LANGCHAIN_ENDPOINT=https://api.smith.langchain.com\n",
    "* LANGCHAIN_API_KEY=your_langchain_api_key\n",
    "* LANGCHAIN_PROJECT=your_project_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a63ff6-99ff-4629-b965-547d12a99ba6",
   "metadata": {},
   "source": [
    "We will call our LangSmith project **007-main-ops-lcel-chain**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b3c1ba-c90e-4360-930b-d2ff39296e4a",
   "metadata": {},
   "source": [
    "## Connect with the .env file located in the same directory of this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93eaa23c-5e9b-4098-9aa4-9950807d1ce4",
   "metadata": {},
   "source": [
    "If you are using the pre-loaded poetry shell, you do not need to install the following package because it is already pre-loaded for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5fca6e-8eed-4ac3-abf8-e96862671a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "id": "992ec4a9-aa01-4e44-aeb9-b9a1f3aa9e54",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T05:21:26.575649Z",
     "start_time": "2025-06-15T05:21:26.566273Z"
    }
   },
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "openai_api_key = os.environ[\"OPENAI_API_KEY\"]"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "6c01d18b-f9f0-427b-a9dc-3d1885160578",
   "metadata": {},
   "source": [
    "#### Install LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e0c516-fb73-4604-be63-c9d432411be0",
   "metadata": {},
   "source": [
    "If you are using the pre-loaded poetry shell, you do not need to install the following package because it is already pre-loaded for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62afa07-b12c-45d4-a99e-04fe68f7f8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed746499-d1b8-41e5-b131-270cf5fa229b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain lanchain-community"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2af3ef-c2c7-445f-92bd-a29c68abce25",
   "metadata": {},
   "source": [
    "## Connect with an LLM and start a conversation with it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb756f6-05a3-41d7-9c58-ef9f02f590c1",
   "metadata": {},
   "source": [
    "If you are using the pre-loaded poetry shell, you do not need to install the following package because it is already pre-loaded for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3fa7337f-3d60-4ede-bdf8-aa7a5cffec43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain-openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce3551e-95ca-41a1-8810-89c495bf93ab",
   "metadata": {},
   "source": [
    "* For this project, we will use OpenAI's gpt-3.5-turbo"
   ]
  },
  {
   "cell_type": "code",
   "id": "9afcbc7d-a816-41e3-925f-850883f5770d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T05:21:46.790244Z",
     "start_time": "2025-06-15T05:21:45.958293Z"
    }
   },
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "aa7f207a-fdb9-46cf-b1c0-06fbcf9b5e99",
   "metadata": {},
   "source": [
    "## Coercion: a chain inside another chain\n",
    "* Remember: almost any component in LangChain (prompts, models, output parsers, etc) can be used as a Runnable.\n",
    "* **Runnables can be chained together using the pipe operator `|`. The resulting chains of runnables are also runnables themselves**."
   ]
  },
  {
   "cell_type": "code",
   "id": "51ba28a5-a605-482a-b3df-43a70e4dc352",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T05:22:06.232856Z",
     "start_time": "2025-06-15T05:22:06.179897Z"
    }
   },
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"tell me a sentence about {politician}\")\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "chain = prompt | model | StrOutputParser()"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "1d91592b-027a-4ea4-a764-94bf27b8610f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T05:22:12.212848Z",
     "start_time": "2025-06-15T05:22:10.939721Z"
    }
   },
   "source": [
    "chain.invoke(\"Chamberlain\")"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Neville Chamberlain is best known for his role as the British Prime Minister who pursued a policy of appeasement towards Nazi Germany in the lead-up to World War II, famously declaring that he had secured \"peace for our time\" after the Munich Agreement in 1938.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "82645057-4570-4dd9-ad3b-58f0365bb4eb",
   "metadata": {},
   "source": [
    "#### Coercion: combine a chain with other Runnables to create a new chain.\n",
    "* See how in the `composed_chain` we are including the previous `chain`:"
   ]
  },
  {
   "cell_type": "code",
   "id": "9c8f1937-227a-4d4f-9e62-2b6792f00541",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T05:26:17.594226Z",
     "start_time": "2025-06-15T05:26:17.579771Z"
    }
   },
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "historian_prompt = ChatPromptTemplate.from_template(\"Was {politician} positive for Humanity?\")\n",
    "\n",
    "composed_chain = {\"politician\": chain} | historian_prompt | model | StrOutputParser()"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "f98a3705-a6b0-473f-b4b8-7edd2dff9831",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T05:26:24.758256Z",
     "start_time": "2025-06-15T05:26:20.099700Z"
    }
   },
   "source": [
    "composed_chain.invoke({\"politician\": \"Lincoln\"})"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Abraham Lincoln is widely regarded as a pivotal figure in American history, and many consider his contributions to humanity to be overwhelmingly positive. His leadership during the Civil War helped preserve the Union and ensure that the principles of democracy and liberty endured in the United States. His efforts to abolish slavery, particularly through the Emancipation Proclamation, which declared the freedom of all enslaved people in the Confederate states, marked a significant step toward ending slavery in America and laid the groundwork for the eventual passage of the 13th Amendment, which abolished slavery nationwide.\\n\\nLincoln\\'s commitment to equality and human rights has had a lasting impact, influencing civil rights movements and discussions about justice and liberty well beyond his time. However, like any historical figure, interpretations of his presidency and policies can vary, and criticisms exist, particularly concerning his views on race and the extent of his commitment to full equality for African Americans.\\n\\nOverall, Lincoln\\'s legacy is often viewed as a force for progress and a significant step toward a more just and equitable society, contributing positively to humanity by advocating for freedom and the principle of government \"of the people, by the people, for the people.\"'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "b650952f-a0b1-404e-ac63-1a11ed4f347e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T05:26:50.670332Z",
     "start_time": "2025-06-15T05:26:44.485869Z"
    }
   },
   "source": [
    "composed_chain.invoke({\"politician\": \"Attila\"})"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Attila the Hun, who ruled in the 5th century, is often viewed as a formidable military leader and a significant figure in the history of the Roman Empire. His impact on the Roman Empire was both significant and complex, and opinions about his legacy vary widely.\\n\\nFrom a historical perspective, Attila was known for his impressive military strategies and his ability to unite various tribes under his leadership. He led numerous successful campaigns against the Eastern and Western Roman Empires, which contributed to the eventual decline of Roman power in the West. His invasions instilled fear and forced the Romans to adapt their military tactics and political strategies.\\n\\nHowever, assessing whether Attila was \"positive for humanity\" is subjective and depends on one’s perspective. Here are some points to consider:\\n\\n### Negative Aspects:\\n1. **Destruction and Violence**: Attila\\'s campaigns were marked by significant violence, destruction, and loss of life. His invasions led to the conquest of territories, looting of cities, and widespread suffering among civilians.\\n2. **Instability**: The destabilization caused by his invasions contributed to the eventual fall of the Western Roman Empire, leading to a period of chaos and fragmentation in Europe known as the Dark Ages.\\n\\n### Positive Aspects:\\n1. **Cultural Exchange**: The movements of the Huns and other tribes during this period eventually led to cultural exchanges between different groups, which could be seen as a pathway to the eventual formation of new nations and cultures in Europe.\\n2. **Military Innovation**: Attila\\'s tactics and military organization influenced future military leaders and strategies, which played a role in shaping the evolution of warfare.\\n\\nUltimately, while Attila the Hun is viewed as a powerful leader with significant military accomplishments, his impact was largely negative due to the violence and suffering inflicted during his campaigns. His legacy is complex, and while he may be seen as a catalyst for change in some respects, the immediate effects of his leadership were devastating for many people. Therefore, whether he is viewed as positive for humanity depends largely on the lens through which one examines his legacy.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "id": "2e4cebf2-c24f-4bb5-bb8f-bcd2daf72f82",
   "metadata": {},
   "source": [
    "## Another example: a chain inside another chain"
   ]
  },
  {
   "cell_type": "code",
   "id": "172fd045-567e-40bc-8e42-e8f3923ca1a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T05:30:34.919605Z",
     "start_time": "2025-06-15T05:30:33.806051Z"
    }
   },
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "prompt1 = ChatPromptTemplate.from_template(\"what is the country {politician} is from?\")\n",
    "prompt2 = ChatPromptTemplate.from_template(\n",
    "    \"what continent is the country {country} in? respond in {language}\"\n",
    ")\n",
    "\n",
    "model = ChatOpenAI()\n",
    "\n",
    "chain1 = prompt1 | model | StrOutputParser()\n",
    "\n",
    "chain2 = (\n",
    "    {\"country\": chain1, \"language\": itemgetter(\"language\")}\n",
    "    | prompt2\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "chain2.invoke({\"politician\": \"Miterrand\", \"language\": \"French\"})"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Miterrand était originaire de l'Europe, plus précisément de la France.\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "id": "5b7e51df-c2fc-47d5-85d4-c4717bf32603",
   "metadata": {},
   "source": [
    "## Fallback for Chains\n",
    "* When working with language models, you may often encounter issues from the underlying APIs, whether these be rate limiting or downtime. Therefore, as you go to move your LLM applications into production it becomes more and more important to safeguard against these. That's why LangChain introduced the concept of fallbacks.\n",
    "* A fallback is an alternative plan that may be used in an emergency.\n",
    "* Fallbacks can be applied not only on the LLM level but on the whole runnable level. This is important because often times different models require different prompts. So if your call to OpenAI fails, you don't just want to send the same prompt to Anthropic - you probably want to use a different prompt template and send a different version there.\n",
    "* We can create fallbacks for LCEL chains. Here we do that with two different models: ChatOpenAI (with a bad model name to easily create a chain that will error) and then normal OpenAI (which does not use a chat model). Because OpenAI is NOT a chat model, you likely want a different prompt."
   ]
  },
  {
   "cell_type": "code",
   "id": "509b9f90-648c-48da-acbf-206519e74966",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T05:33:00.678387Z",
     "start_time": "2025-06-15T05:33:00.674523Z"
    }
   },
   "source": [
    "# First let's create a chain with a ChatModel\n",
    "# We add in a string output parser here so the outputs between the two are the same type\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You're a funny assistant who always includes a joke in your response\",\n",
    "        ),\n",
    "        (\"human\", \"Who is the best {sport} player worldwide?\"),\n",
    "    ]\n",
    ")\n",
    "# Here we're going to use a bad model name to easily create a chain that will error\n",
    "chat_model = ChatOpenAI(model=\"gpt-fake\")\n",
    "\n",
    "bad_chain = chat_prompt | chat_model | StrOutputParser()"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "id": "5d75321e-ec9c-41a3-9377-1b26d7ced7d9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T05:33:08.113432Z",
     "start_time": "2025-06-15T05:33:08.083062Z"
    }
   },
   "source": [
    "# Now lets create a chain with the normal OpenAI model\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "prompt_template = \"\"\"Instructions: You're a funny assistant who always includes a joke in your response.\n",
    "\n",
    "Question: Who is the best {sport} player worldwide?\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(prompt_template)\n",
    "\n",
    "llm = OpenAI()\n",
    "\n",
    "good_chain = prompt | llm"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "id": "30b6231c-ef6e-4d18-ba22-b92888e515cc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T05:34:32.974953Z",
     "start_time": "2025-06-15T05:34:31.823243Z"
    }
   },
   "source": [
    "# We can now create a final chain which combines the two\n",
    "chain = bad_chain.with_fallbacks([good_chain])\n",
    "\n",
    "chain.invoke({\"sport\": \"curling\"})"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\nAnswer: I'm not sure, but I do know that they must have some serious stone-cold skills!\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "id": "5d99ae33-c4e7-42eb-b65e-82ce3260f9c3",
   "metadata": {},
   "source": [
    "## How to execute the code from Visual Studio Code\n",
    "* In Visual Studio Code, see the file 007-main-ops-lcel-chain.py\n",
    "* In terminal, make sure you are in the directory of the file and run:\n",
    "    * python 007-main-ops-lcel-chain.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0688402d-d575-4339-b3e8-35da12e89e1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
